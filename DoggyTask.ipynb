{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DoggyTask.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LftDsA7D1vcn",
        "outputId": "3dff496a-d156-4807-d2d0-84da2b94c4ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4BBHlX9rERk",
        "outputId": "36234f07-e16e-411c-d7b6-9c09b64684db"
      },
      "source": [
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psgs93OArbC3",
        "outputId": "126742d8-c718-4a93-9430-8ec1a224143b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'=1.6.0'\n",
            " 1712396_AssignmentML\n",
            " 1712396_BT1_KiemTraHamSinhSo.ipynb\n",
            " 1712396_BT2_MNIST-Classification-pytorch.ipynb\n",
            " 1712396_BTL_GANforCIFAR_10.ipynb\n",
            " 1712396_BTL_VAEforCIFAR_10.ipynb\n",
            " annotations\n",
            " bottom-up-feat.ipynb\n",
            " data1M300d.npz\n",
            " demo_faster_rcnn.ipynb\n",
            " demo.ipynb\n",
            " DoggyTask.ipynb\n",
            " en-vi.ipynb\n",
            " envi-model.avg-250000.tar.xz\n",
            " en.wiki.bpe.vs10000.model\n",
            " glove.6B.100d.txt\n",
            " glove.6B.200d.txt\n",
            " glove.6B.300d.txt\n",
            " glove.6B.50d.txt\n",
            " glove.6B.zip\n",
            " gnmt_en_vi_u512\n",
            " ImagePresentation.ipynb\n",
            " MNIST-cnn-pytorch.ipynb\n",
            " model_en_vi_epoch19_9.12.t7\n",
            " netflix_titles.csv.gsheet\n",
            " nmt\n",
            " NMT_icode.ipynb\n",
            " output2021-03-16.csv.gsheet\n",
            " sentencepiece\n",
            " t2t_export\n",
            " Task1WhattowatchonNetflix.ipynb\n",
            " test-2013-en-vi.tgz\n",
            " train_answer.txt\n",
            " train_en.txt\n",
            " training_checkpoints\n",
            " train_question.txt\n",
            " train_vi.txt\n",
            " translate.ipynb\n",
            " TranslationEn-Vi.ipynb\n",
            " Translation.ipynb\n",
            " tst2013.en\n",
            " tst2013.vi\n",
            " Tut1-Pytorch.ipynb\n",
            " Tut2.ipynb\n",
            " Tut3.ipynb\n",
            " Tut4.ipynb\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            " Untitled2.ipynb\n",
            " Untitled3.ipynb\n",
            " vie.txt\n",
            " vqa_raw_train_v1.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsS5mWXOvROD"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY0UrUQ9r8CV"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import string \n",
        "import os\n",
        "import re\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5brA8PRtQlJ"
      },
      "source": [
        "embedding_dict = dict()\n",
        "f = open('glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coef = np.asarray(values[1:],dtype='float32')\n",
        "    embedding_dict[word] = coef # save word as key and coefficients as the value in the dictionary\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqHE-tmj8p9f"
      },
      "source": [
        "def preprocess(file):\n",
        "    processed = [] # preprocess the file\n",
        "    lines = file.read().split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.lower() # lower case\n",
        "        line = line.replace('#','<num>') # replace the token '#' with '<num>'\n",
        "        line = re.sub(\">.<\",' point ',line) # replace the token '>.<' with ' point '\n",
        "        text = re.sub(r\"[^a-z?.!,'<>]+\",\" \",line) # replace other tokens with a space\n",
        "        text = text.rstrip().strip() # strip white space\n",
        "        text = 'bos ' + text + ' eos' # beginning and end tokens for each sentence\n",
        "        processed.append(text)\n",
        "    return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "cB6V-WzR8xY2",
        "outputId": "e814137f-2d08-4aad-a5ae-ecc53d531767"
      },
      "source": [
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open('output2021-03-16.csv').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "rows = rows[1:]\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "pd.DataFrame.from_records(rows)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When was the Vat formally opened?</td>\n",
              "      <td>Formally established in 1475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is the library for?</td>\n",
              "      <td>he Vatican Library is a research library</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>for what subjects?</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and?</td>\n",
              "      <td>Vatican Library is a research library for hist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what was started in 2014?</td>\n",
              "      <td>March 2014, the Vatican Library began an initi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108642</th>\n",
              "      <td>Who was a sub?</td>\n",
              "      <td>substitute Xabi Alonso</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108643</th>\n",
              "      <td>Was it his first game this year?</td>\n",
              "      <td>Xabi Alonso made his first appearance of the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108644</th>\n",
              "      <td>What position did the team reach?</td>\n",
              "      <td>Real moved up to third in the table</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108645</th>\n",
              "      <td>Who was ahead of them?</td>\n",
              "      <td>six points behind Barca.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108646</th>\n",
              "      <td>By how much?</td>\n",
              "      <td>six points behind Barca.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>108647 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        0                                                  1\n",
              "0       When was the Vat formally opened?                       Formally established in 1475\n",
              "1                what is the library for?           he Vatican Library is a research library\n",
              "2                      for what subjects?  Vatican Library is a research library for hist...\n",
              "3                                    and?  Vatican Library is a research library for hist...\n",
              "4               what was started in 2014?  March 2014, the Vatican Library began an initi...\n",
              "...                                   ...                                                ...\n",
              "108642                     Who was a sub?                             substitute Xabi Alonso\n",
              "108643   Was it his first game this year?  Xabi Alonso made his first appearance of the s...\n",
              "108644  What position did the team reach?                Real moved up to third in the table\n",
              "108645             Who was ahead of them?                           six points behind Barca.\n",
              "108646                       By how much?                           six points behind Barca.\n",
              "\n",
              "[108647 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29otHm38GXC9"
      },
      "source": [
        "lines_train_question = np.array(rows)[:,0]\n",
        "lines_train_answer = np.array(rows)[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41KgCO0AAKZ_"
      },
      "source": [
        "TRAIN_ques_len = []\n",
        "TRAIN_ans_len = [] \n",
        "\n",
        "for line in lines_train_question:\n",
        "    w = 0\n",
        "    for word in line.split(' '):\n",
        "        w += 1\n",
        "    TRAIN_ques_len.append(w)\n",
        "\n",
        "for line in lines_train_answer:\n",
        "    w = 0\n",
        "    for word in line.split(' '):\n",
        "        w += 1\n",
        "    TRAIN_ans_len.append(w)\n",
        "TRAIN_ques_len = np.array(TRAIN_ques_len)\n",
        "TRAIN_ans_len = np.array(TRAIN_ans_len) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ksuNKOEA2T",
        "outputId": "e6d07acc-480a-4357-da1e-39cd188b8867"
      },
      "source": [
        "print(\"\\t\\t Min\\t 25%\\t Avg\\t 75%\\t Max\")\n",
        "print(\"\\nTRAIN Input\\t {}\\t{}\\t{}\\t{}\\t{}\".format(np.percentile(TRAIN_ques_len,0),\n",
        "                                                  np.percentile(TRAIN_ques_len,25),\n",
        "                                                  np.percentile(TRAIN_ques_len,50),\n",
        "                                                  np.percentile(TRAIN_ques_len,75),\n",
        "                                                  np.percentile(TRAIN_ques_len,100)))\n",
        "print(\"TRAIN Summary\\t {}\\t{}\\t{}\\t{}\\t{}\".format(np.percentile(TRAIN_ans_len,0),\n",
        "                                                  np.percentile(TRAIN_ans_len,25),\n",
        "                                                  np.percentile(TRAIN_ans_len,50),\n",
        "                                                  np.percentile(TRAIN_ans_len,75),\n",
        "                                                  np.percentile(TRAIN_ans_len,100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t Min\t 25%\t Avg\t 75%\t Max\n",
            "\n",
            "TRAIN Input\t 1.0\t4.0\t5.0\t7.0\t45.0\n",
            "TRAIN Summary\t 1.0\t3.0\t7.0\t12.0\t353.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmaRWUYhIXep",
        "outputId": "e2c6f6b5-db96-4a55-eb21-5d4dd6a481a9"
      },
      "source": [
        "words = [] \n",
        "for line in lines_train_question:\n",
        "    for word in line.split(' '):\n",
        "        words.append(word)\n",
        "for line in lines_train_answer:\n",
        "    for word in line.split(' '):\n",
        "        words.append(word)\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "embed_dim = 300\n",
        "words_found = 0\n",
        "words_OOV = 0\n",
        "word2idx = {}\n",
        "word2idx['<pad>'] = 0\n",
        "# convert words to index by creating word2idx dictionary\n",
        "for index, word in enumerate(vocab):\n",
        "    word2idx[word] = index+1\n",
        "# create weight embedding matrix for GloVe embedding\n",
        "weight_embedding = np.zeros([len(vocab)+1,embed_dim],dtype='float32')\n",
        "for i, word in enumerate(vocab):\n",
        "    try:\n",
        "        # if word found in pretrained GloVe embedding dictionary\n",
        "        weight_embedding[i,:] = embedding_dict[word]\n",
        "        words_found += 1\n",
        "    except:\n",
        "        # if word not found, initialize as random vector\n",
        "        weight_embedding[i,:] = np.random.normal(scale=0.6, size=[embed_dim,])\n",
        "        words_OOV += 1\n",
        "\n",
        "print(\"{} words found\".format(words_found))\n",
        "print(\"{} OOV words\".format(words_OOV))\n",
        "idx2word = dict((v,k) for k,v in word2idx.items())\n",
        "print(\"{} words in the vocabulary\".format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26435 words found\n",
            "76799 OOV words\n",
            "103234 words in the vocabulary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyfICPe0JLR9",
        "outputId": "1d1a81d0-10b7-4c17-b0d6-88041984f8c6"
      },
      "source": [
        "ques_ints = []\n",
        "ans_ints = []\n",
        "\n",
        "for each in lines_train_question:\n",
        "    sentence = []\n",
        "    for word in each.split():\n",
        "        sentence.extend([word2idx[word]])\n",
        "    ques_ints.append(sentence)\n",
        "\n",
        "for each in lines_train_answer:\n",
        "    sentence = []\n",
        "    for word in each.split():\n",
        "        if word in word2idx:\n",
        "          sentence.extend([word2idx[word]])\n",
        "        else:\n",
        "          word = word.lower()\n",
        "          if word in word2idx:\n",
        "            sentence.extend([word2idx[word]])\n",
        "          else:\n",
        "            if word == 'eos':\n",
        "              sentence.extend([word2idx['end']])\n",
        "            else:\n",
        "              sentence.extend([word2idx['unknown']])\n",
        "    ans_ints.append(sentence)\n",
        "\n",
        "ques_len = 50 \n",
        "ans_len = 14\n",
        "ques_feature = np.zeros((len(ques_ints),ques_len),dtype=int)\n",
        "ans_feature = np.zeros((len(ans_ints),ans_len), dtype=int)\n",
        "for i, row in enumerate(ques_ints):\n",
        "    ques_feature[i,:len(row)] = np.array(row)[:ques_len]\n",
        "for i, row in enumerate(ans_ints):\n",
        "    ans_feature[i,:len(row)] = np.array(row)[:ans_len]\n",
        "\n",
        "# train test split\n",
        "split_frac = 0.85 \n",
        "split_index = int(len(ques_feature)*0.85)\n",
        "train_ques, test_ques = ques_feature[:split_index],ques_feature[split_index:]\n",
        "train_ans, test_ans = ans_feature[:split_index],ans_feature[split_index:]\n",
        "\n",
        "print(\"\\t\\t\\tData Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_ques.shape), \n",
        "      \"\\nTest set: \\t\\t{}\".format(test_ques.shape))\n",
        "\n",
        "print(\"\\n\\n\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_ans.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_ans.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tData Shapes:\n",
            "Train set: \t\t(92349, 50) \n",
            "Test set: \t\t(16298, 50)\n",
            "\n",
            "\n",
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(92349, 14) \n",
            "Test set: \t\t(16298, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BM0Ne5tOEnH",
        "outputId": "29c296b7-4fc7-421d-ce93-53ef0541caf0"
      },
      "source": [
        "np.savez('data1M300d.npz',train_ques=train_ques,test_ques=test_ques,\n",
        "         train_ans=train_ans,test_ans=test_ans,\n",
        "         word2idx = word2idx, idx2word=idx2word, \n",
        "         embed_matrix=weight_embedding, output_length = TRAIN_ans_len)\n",
        "print(\"Data Saved!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5dU1liHOxyr"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import string \n",
        "import os\n",
        "import re\n",
        "import queue\n",
        "%matplotlib inline\n",
        "\n",
        "# train_ques = data\n",
        "# test_ques = data['test_ques']\n",
        "# train_summary = data['train_summary']\n",
        "# test_summary = data['test_summary']\n",
        "# word2idx = data['word2idx']\n",
        "# word2idx = dict(word2idx.item())\n",
        "# idx2word = data['idx2word']\n",
        "# idx2word = dict(idx2word.item())\n",
        "# weight_embedding = data['embed_matrix']\n",
        "# weight_embedding = torch.from_numpy(weight_embedding)\n",
        "# output_length = data['output_length']\n",
        "split_index = int(len(test_ques)*0.1)\n",
        "val_ques, test_ques  = test_ques[:split_index],test_ques[split_index:]\n",
        "val_ans, test_ans = test_ans[:split_index],test_ans[split_index:]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device to run the code on GPU\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8j_MbzmO01l"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def create_emb(weight_matrix, non_trainable=False):\n",
        "    # create embedding matrix with the pretrained GloVe embedding matrix\n",
        "    emb_layer = torch.nn.Embedding(weight_matrix.shape[0],weight_matrix.shape[1])\n",
        "    emb_layer.load_state_dict({'weight': weight_matrix})\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False # turn off training for the embedding vector\n",
        "    return emb_layer\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,kernel_size,filter_size,dropout,num_hidden,layers,weight_matrix,embed_dim,device):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.layers = layers\n",
        "        self.kernel_size = kernel_size\n",
        "        self.filter_size = filter_size\n",
        "        self.num_hidden = num_hidden\n",
        "        self.device = device\n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # convolutional layers\n",
        "        self.conv1 = torch.nn.Conv1d(embed_dim,self.filter_size,self.kernel_size[0],stride=1,padding=0)\n",
        "        self.conv2 = torch.nn.Conv1d(embed_dim,self.filter_size,self.kernel_size[1],stride=1,padding=1)\n",
        "        self.conv3 = torch.nn.Conv1d(embed_dim,self.filter_size,self.kernel_size[2],stride=1,padding=2)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.embedding = create_emb(weight_matrix,True)\n",
        "        # encoder LSTM\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.embed_dim,hidden_size = self.num_hidden,\n",
        "                                  num_layers = self.layers,batch_first=True,dropout=0.5,\n",
        "                                 bidirectional=True)\n",
        "    def forward(self,x,hidden):\n",
        "        x = self.embedding(x) # embed the input text\n",
        "        \n",
        "        # convolutional layers\n",
        "        #x1 = torch.tanh(self.dropout(self.conv1(x)))\n",
        "        #x2 = torch.tanh(self.dropout(self.conv2(x)))\n",
        "        #x3 = torch.tanh(self.dropout(self.conv3(x)))\n",
        "        # apply dropout\n",
        "        lstm_in = self.dropout((x))\n",
        "        # encoder LSTM\n",
        "        output,(h_hidden,c_hidden) = self.lstm(lstm_in,hidden)\n",
        "        # hidden state of the encoder (in case of multilayer LSTM)\n",
        "        h = torch.cat((h_hidden[-1,:,:],h_hidden[-2,:,:]),1)\n",
        "        c = torch.cat((c_hidden[-1,:,:],c_hidden[-2,:,:]),1)\n",
        "\n",
        "        hidden = (h.unsqueeze(0),c.unsqueeze(0))\n",
        "       \n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self,batch_size):\n",
        "        # initialize the hidden state as the zero tensor\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.layers*2,batch_size, self.num_hidden).zero_(),\n",
        "                  weight.new(self.layers*2, batch_size, self.num_hidden).zero_())\n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y826vLTnPz2Z"
      },
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self,num_hidden,dropout,vocab_size,layers,weight_matrix,embed_dims,device):\n",
        "        super(AttentionDecoder,self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.dropout = dropout\n",
        "        self.layers = layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dims = embed_dims\n",
        "        self.device = device\n",
        "        self.dropout_layer = torch.nn.Dropout(self.dropout)\n",
        "        \n",
        "        # Linear layers for the attention mechanism\n",
        "        self.V = torch.nn.Linear(self.num_hidden,1)\n",
        "        \n",
        "        # layer to get the pointer probability\n",
        "        self.generator = torch.nn.Linear(2*self.num_hidden+self.embed_dims,1)\n",
        "        # output layer to vocab\n",
        "        self.output_layer = torch.nn.Linear(2*self.num_hidden,self.vocab_size)\n",
        "        # softmax function\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        # decoder LSTM\n",
        "        self.lstm = torch.nn.LSTM(input_size = self.num_hidden+self.embed_dims,hidden_size = self.num_hidden,\n",
        "                                  num_layers = self.layers,batch_first=True,dropout=0.5,\n",
        "                                 bidirectional=False)\n",
        "        # embedding matrix\n",
        "        self.embedding = create_emb(weight_matrix,True)\n",
        "        self.sig = torch.nn.Sigmoid()\n",
        "        self.device = device\n",
        "        \n",
        "        \n",
        "    def forward(self,x,enc_out,hidden,text,batch_size):\n",
        "        # decoder\n",
        "        # Decoder Input Shape: [batch_size]\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        # Decoder Embedded Shape: [batch_size,1,embed_dim]\n",
        "        x = self.dropout_layer(x)\n",
        "        \n",
        "        # Bahdanau Attention\n",
        "        dec_a = hidden[0].permute(1,0,2)\n",
        "        enc_score = self.V(torch.tanh(enc_out + dec_a)) # attention score\n",
        "        # Attention Score Shape: [batch_size,input_seq_len,1]\n",
        "        enc_weight = self.softmax(enc_score) # attention weight\n",
        "        # Attention Weight Shape: [batch_size,input_seq_len,1]\n",
        "        enc_context = torch.mul(enc_weight,enc_out) # find the context vector\n",
        "        # Attention Context Shape: [batch_size,input_seq_len,decoder_num_hidden]\n",
        "        enc_context = enc_context.sum(1)\n",
        "        # Attention Context Shape: [batch_size,decoder_num_hidden]\n",
        "        enc_context.unsqueeze_(1)\n",
        "        # Attention Context Shape: [batch_size,1,decoder_num_hidden]     \n",
        "        \n",
        "        d_in = torch.cat((x,enc_context),2)\n",
        "        # Decoder Input Shape: [batch_size,1,decoder_num_hidden+embed_dims]  \n",
        "        \n",
        "        # run the decoder LSTM\n",
        "        d_output, hidden = self.lstm(d_in,hidden)\n",
        "        # Decoder Output Shape: [batch_size,1,decoder_num_hidden]  \n",
        "        # Decoder Hidden Shape: [1,batch_size,decoder_num_hidden]  \n",
        "\n",
        "        # concatenate output with the encoder context tensor\n",
        "        output = torch.cat((d_output.squeeze(1),enc_context.squeeze(1)),1)\n",
        "        # Decoder Output Shape: [batch_size,2*decoder_num_hidden]  \n",
        "        \n",
        "        output_generator = torch.cat((enc_context.squeeze(1),d_output.squeeze(1),x.squeeze(1)),1)\n",
        "        # Generator Input Shape: [batch_size,2*decoder_num_hidden + embed_dims]  \n",
        "        \n",
        "        p_gen = self.sig(self.generator(output_generator).squeeze(1))\n",
        "        \n",
        "        # pointer-generator\n",
        "        p_pointer = 1 - p_gen\n",
        "        pointer_prob = torch.zeros([batch_size,self.vocab_size],device=self.device)\n",
        "        for i in range(batch_size):\n",
        "            pointer_prob[i,text[i,:]] = enc_weight[i,:,0] # pointer probability weights are the attention scores\n",
        "        generator_prob = self.output_layer(output) # output layer to get vocabulary probability\n",
        "        output_probability = torch.mul(p_pointer.unsqueeze(1),pointer_prob) + torch.mul(p_gen.unsqueeze(1),generator_prob)\n",
        "        \n",
        "        return output_probability, hidden\n",
        "    \n",
        "    def init_hidden(self,batch_size):\n",
        "        # initialize the hidden state as the zero tensor\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.layers, batch_size, 2*self.num_hidden).zero_(),\n",
        "                  weight.new(self.layers, batch_size, 2*self.num_hidden).zero_())\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWUOy0DsP3sA"
      },
      "source": [
        "# hyperparameters of the mode\n",
        "kernel_size = [1,3,5] # kernel size for CNN\n",
        "filter_size = 100 # number of kernels used in CNN\n",
        "dropout = 0.5 # dropout probability\n",
        "num_hidden = 256 # number of hidden units in LSTMs\n",
        "enc_layers = 1 # number of layers in the LSTM encoder\n",
        "batch_size = 32\n",
        "vocab_size = len(word2idx) # length of vocab\n",
        "dec_layers = 1 # number of layers in the LSTM decoder\n",
        "embed_dims = 300 # embedding dimensions\n",
        "\n",
        "# create a beam search node to store the running sequences for the beam search decoder\n",
        "# and records the hidden state associated with the sequence, the probability and the loss\n",
        "class BeamNode(object):\n",
        "    def __init__(self,hidden_state,seq,prob,length,loss):\n",
        "        self.hidden = hidden_state\n",
        "        self.seq = seq\n",
        "        self.prob = prob\n",
        "        self.len = length\n",
        "        self.loss = loss\n",
        "        self.s = self.score()\n",
        "        \n",
        "    def score(self):\n",
        "        return self.prob/float(self.len-1+1e-6) # calculates the score of a sequence\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,kernel_size,filter_size,dropout,num_hidden,enc_layers,weight_embedding,\n",
        "                vocab_size,dec_layers,embed_dims,device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.device = device\n",
        "        self.encoder = Encoder(kernel_size,filter_size,dropout,num_hidden,enc_layers,weight_embedding,embed_dims,device)\n",
        "        self.decoder = AttentionDecoder(2*num_hidden,dropout,vocab_size,dec_layers,weight_embedding,embed_dims,device)\n",
        "    \n",
        "    def forward(self,x,target,e_hidden,criterion,batch_size):\n",
        "        # training the decoder\n",
        "        loss = 0\n",
        "        prediction = target[:,0].unsqueeze(1) # records the running sequences generated by the decoder\n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) # run the encoder\n",
        "        d_hidden = enc_hidden # the decoder input hidden state is the encoder output hidden state\n",
        "        dec_input = target[:,0] # the decoder input starts of as the 'bos' token\n",
        "        for t in range(1,target.shape[1]):\n",
        "            # run the decoder\n",
        "            logits, d_hidden = self.decoder(dec_input,enc_output,d_hidden,x,batch_size)\n",
        "            dec_input = target[:,t] # teacher forcing turned on \n",
        "            loss += criterion(logits,target[:,t]) # calculate the loss function\n",
        "            # add to get the running prediction output by the decoder\n",
        "            prediction = torch.cat((prediction,torch.argmax(logits,dim=0).unsqueeze(1)),0)\n",
        "        return loss, prediction\n",
        "    \n",
        "    def inference_greedy(self,x,target,e_hidden,criterion,batch_size):\n",
        "        loss = 0\n",
        "        prediction = target[:,0].unsqueeze(1) # records the running sequences generated by the decoder\n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) # run the encoder\n",
        "        d_hidden = enc_hidden # the decoder input hidden state is the encoder output hidden state\n",
        "        dec_input = target[:,0] # the decoder input starts of as the 'bos' token\n",
        "        for t in range(1,target.shape[1]):\n",
        "            # run the decoder\n",
        "            logits, d_hidden = self.decoder(dec_input,enc_output,d_hidden,x,batch_size)\n",
        "            # the input to the decoder at the next step is the argument with the largest probability\n",
        "            dec_input = torch.argmax(logits,dim=1)\n",
        "            loss += criterion(logits,target[:,t]) # calculate the loss\n",
        "            # add to get the running prediction output by the decoder\n",
        "            prediction = torch.cat((prediction,torch.argmax(logits,dim=0).unsqueeze(1)),0)\n",
        "        return loss, prediction\n",
        "    \n",
        "    def inference_beam(self,x,target,e_hidden,criterion,beam_width,batch_size):\n",
        "        decoded = [] \n",
        "        losses = 0\n",
        "        enc_output,enc_hidden = self.encoder(x,e_hidden) # run the encoder\n",
        "        for i in range(batch_size):\n",
        "            # for each sentence in the batch\n",
        "            prediction = target[i,0].view([1]).unsqueeze(1) # running prediction tensor\n",
        "            dec_hidden = enc_hidden[0].permute(1,0,2)[i,:,:].unsqueeze(0)\n",
        "            dec_input = target[i,0].view([1])\n",
        "            d_hidden = (enc_hidden[0][:,i,:].unsqueeze(0),enc_hidden[1][:,i,:].unsqueeze(0))\n",
        "            first_node = BeamNode(d_hidden,dec_input,0,1,0) # the first node is the 'bos' token\n",
        "            nodes = queue.PriorityQueue(maxsize=beam_width) # create a priority queue to store the beam search nodes\n",
        "            nodes.put((-first_node.score(),first_node)) # place the first node in the queue\n",
        "            for t in range(1,target.shape[1]):\n",
        "                # go through each of the words in the target\n",
        "                candidatenodes = [] # stores the candidate nodes for a one step look ahead\n",
        "                candidatescore = [] # stores the candidate scores for a one step look ahead\n",
        "                donenodes = [] # stores the sequences that have been completed - contain 'eos' token\n",
        "\n",
        "                # This runs through the 10 sequences in the queue and does a one step look ahead\n",
        "                # to find 100 possible candidates. The top 10 candidates are then added to the queue\n",
        "                # and the next word of the decoder is computed\n",
        "                \n",
        "                for _ in range(nodes.qsize()):\n",
        "                    # for each sequence in the queue\n",
        "                    sc,nodex = nodes.get() # get the node from the queue\n",
        "                    seq = nodex.seq.view([-1,1]) # the sequence in the node\n",
        "                    dec_input = seq[-1,:] # the last word of the sequence is the input to the decoder\n",
        "                    hidden = nodex.hidden # the hidden state associated with the sequence\n",
        "                    d_hidden = (hidden[0],hidden[1]) # the hidden state of the decoder\n",
        "  \n",
        "                    if dec_input == word2idx['end']:\n",
        "                        # if the sequence is completed, then it is added to 'donenodes' list\n",
        "                        donenodes.append((sc,nodex))\n",
        "                    else:\n",
        "                        # run the decoder\n",
        "                        logits, d_hidden = self.decoder(dec_input,enc_output[i,:,:].unsqueeze(0),d_hidden,x,batch_size=1)\n",
        "                        # calculate the loss\n",
        "                        loss = criterion(logits,target[i,t].view([1]))\n",
        "                        # pick the top 10 logits values\n",
        "                        log_p, index = torch.topk(logits,beam_width)\n",
        "                        for k in range(beam_width):\n",
        "                            # create a beam node for each of the top 10 to get the candidate nodes\n",
        "                            node = BeamNode(d_hidden, torch.cat([nodex.seq,index[0,k].unsqueeze(0)]),nodex.prob+log_p[0][k],nodex.len+1,nodex.loss+loss)\n",
        "                            score = -node.score()\n",
        "                            candidatenodes.append([score,node])\n",
        "                # put the finished sequences first in the queue first and then fill with candidate nodes\n",
        "                for score,n in donenodes:\n",
        "                    nodes.put((score,n))\n",
        "                for score,n in candidatenodes[nodes.qsize():beam_width]:\n",
        "                    nodes.put((score,n))\n",
        "            _,output_node = nodes.get()\n",
        "            \n",
        "            # the output of the beam search decoder is the sequence with the lowest score\n",
        "            decoded.append(output_node.seq)\n",
        "            losses+=output_node.loss\n",
        "            del nodes\n",
        "        return losses/batch_size,decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCcmj6u2P-3U"
      },
      "source": [
        "def get_batches(x, y,batch_size=100):\n",
        "    n_batches = len(x)//batch_size\n",
        "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "eJHE4ImvQBsX",
        "outputId": "0b957bf7-a74a-4327-8c23-5ed3e39170f8"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 2\n",
        "learning_rate = 0.001\n",
        "# initialize model\n",
        "model = Seq2Seq(kernel_size,filter_size,dropout,num_hidden,enc_layers, torch.from_numpy(weight_embedding),\n",
        "                vocab_size,dec_layers,embed_dims,device).to(device)\n",
        "# Adam Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "# cross entropy loss function with the padding ignored\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "counter = 0\n",
        "loss = []\n",
        "beam_width = 10 # beam search width is 10\n",
        "for e in range(epochs):\n",
        "    start_time = time.time() # start timer\n",
        "    # shuffle the data at the beginning of epoch\n",
        "    p = np.random.permutation(train_ans.shape[0])\n",
        "    train_ques = train_ques[p,:]\n",
        "    train_ans = train_ans[p,:]\n",
        "    # initialize encoder initial hidden state\n",
        "    e_hidden = model.encoder.init_hidden(batch_size)\n",
        "    \n",
        "    for x,y in get_batches(train_ques,train_ans,batch_size):\n",
        "        model.train()\n",
        "        # convert inputs to PyTorch tensor\n",
        "        x = torch.from_numpy(x).to(device)\n",
        "        y = torch.from_numpy(y).to(device)\n",
        "        \n",
        "        e_hidden = tuple([each.data for each in e_hidden])\n",
        "        optimizer.zero_grad() # zero the gradients of the model\n",
        "        # train the model\n",
        "        l,prediction = model(x,y,e_hidden,criterion,batch_size)\n",
        "        loss.append(l.item()) # add loss function\n",
        "        l.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),2) # gradient clip with norm = 2\n",
        "        optimizer.step() # step the optimizer to apply the gradient\n",
        "        \n",
        "        if counter%50 == 0:\n",
        "            print(\"Epoch: {}/{} \".format(e+1, epochs),\n",
        "                      \"\\tStep: {} \".format(counter),\n",
        "                      \"\\tLoss: {:.4f} \".format(l.item()))\n",
        "        counter += 1\n",
        "    print(\"Time to train epoch: {0} s\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2  \tStep: 0  \tLoss: 150.0630 \n",
            "Epoch: 1/2  \tStep: 50  \tLoss: 111.5572 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2b9ec585e222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradients of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3727c17e66f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target, e_hidden, criterion, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdec_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# teacher forcing turned on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;31m# add to get the running prediction output by the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWL-00K8XHnm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iZG0t9qQFBe",
        "outputId": "33669e90-4d7b-4f4a-dddc-2fa2ce3257e5"
      },
      "source": [
        "with torch.no_grad():\n",
        "    beam_time = time.time() # start timer\n",
        "    loss_beam = [] \n",
        "    beam_predict = [] # save beam search decoder outputs\n",
        "    ans_validation = [] # save validation target summaries\n",
        "    ques_validation = [] # save validation input text\n",
        "    model.eval()\n",
        "    # initialize the encoder hidden state\n",
        "    val_hidden = model.encoder.init_hidden(batch_size)\n",
        "    for x_val, y_val in get_batches(val_ques,val_ans,batch_size):\n",
        "        # convert data to PyTorch tensor\n",
        "        x_val = torch.from_numpy(x_val).to(device)\n",
        "        y_val = torch.from_numpy(y_val).to(device)\n",
        "        val_hidden = tuple([each.data for each in val_hidden])\n",
        "        # run the beam search decoder\n",
        "        val_loss, prediction = model.inference_beam(x,y,val_hidden,criterion,beam_width,batch_size=32)\n",
        "        print(prediction)\n",
        "        loss_beam.append(val_loss.item())\n",
        "        beam_predict.append(prediction)\n",
        "        ans_validation.append(y_val)\n",
        "        ques_validation.append(x_val)\n",
        "        break\n",
        "    model.train()\n",
        "    print(\"Beam Test: {0} s\".format(time.time()-beam_time))\n",
        "    print(\"Val Beam Search Loss: {:.4f}\".format(np.mean(loss_beam)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([82952,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1], device='cuda:0'), tensor([7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), tensor([9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), tensor([3627,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([690,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0'), tensor([91313,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1], device='cuda:0'), tensor([2063,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([83,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "       device='cuda:0'), tensor([11982,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1], device='cuda:0'), tensor([4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), tensor([3432,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([2701,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([125,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0'), tensor([69665,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1], device='cuda:0'), tensor([13166,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1], device='cuda:0'), tensor([77,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "       device='cuda:0'), tensor([3461,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([20,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "       device='cuda:0'), tensor([2302,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([8373,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([5250,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([625,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0'), tensor([9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), tensor([760,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0'), tensor([13811,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1], device='cuda:0'), tensor([296,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0'), tensor([20,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "       device='cuda:0'), tensor([27,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1],\n",
            "       device='cuda:0'), tensor([5570,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0'), tensor([108,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
            "       device='cuda:0'), tensor([5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), tensor([1226,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1], device='cuda:0')]\n",
            "Beam Test: 20.516160249710083 s\n",
            "Val Beam Search Loss: 56.7449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpPim6GSQ-SS",
        "outputId": "86e2c916-a558-48a7-da1a-c7b99bfb3b95"
      },
      "source": [
        "beam = []\n",
        "file = open(\"beam.txt\",\"w\")\n",
        "for i in beam_predict:\n",
        "    for p in i:\n",
        "        array = torch.Tensor.cpu(p).numpy()\n",
        "        beam_string = ''\n",
        "        for q in array:\n",
        "            beam_string += idx2word[q] + ' ' \n",
        "        file.write(beam_string+'\\n')\n",
        "        beam.append(beam_string)\n",
        "file.close()\n",
        "\n",
        "# save the validation summaries for the input text used in the beam search decoder\n",
        "summary = []\n",
        "file = open(\"validation_answer.txt\",\"w\")\n",
        "for i in ans_validation:\n",
        "    for p in i:\n",
        "        array = torch.Tensor.cpu(p).numpy()\n",
        "        summary_string = ''\n",
        "        for q in array:\n",
        "            if q != 0:\n",
        "                summary_string += idx2word[q] + ' ' \n",
        "        file.write(summary_string+'\\n')\n",
        "        summary.append(summary_string)\n",
        "file.close()\n",
        "\n",
        "# Save the validation input text as a .txt file\n",
        "text = []\n",
        "file = open(\"validation_question.txt\",\"w\")\n",
        "for i in ques_validation:\n",
        "    for p in i:\n",
        "        array = torch.Tensor.cpu(p).numpy()\n",
        "        text_string = ''\n",
        "        for q in array:\n",
        "            if q != 0:\n",
        "                text_string += idx2word[q] + ' ' \n",
        "        file.write(text_string+'\\n')\n",
        "        text.append(text_string)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deepika \n",
            "Deepika the \n",
            "Deepika the the \n",
            "Deepika the the the \n",
            "Deepika the the the the \n",
            "Deepika the the the the the \n",
            "Deepika the the the the the the \n",
            "Deepika the the the the the the the \n",
            "Deepika the the the the the the the the \n",
            "Deepika the the the the the the the the the \n",
            "Deepika the the the the the the the the the the \n",
            "Deepika the the the the the the the the the the the \n",
            "Deepika the the the the the the the the the the the the \n",
            "Deepika the the the the the the the the the the the the the \n",
            "and \n",
            "and the \n",
            "and the the \n",
            "and the the the \n",
            "and the the the the \n",
            "and the the the the the \n",
            "and the the the the the the \n",
            "and the the the the the the the \n",
            "and the the the the the the the the \n",
            "and the the the the the the the the the \n",
            "and the the the the the the the the the the \n",
            "and the the the the the the the the the the the \n",
            "and the the the the the the the the the the the the \n",
            "and the the the the the the the the the the the the the \n",
            "he \n",
            "he the \n",
            "he the the \n",
            "he the the the \n",
            "he the the the the \n",
            "he the the the the the \n",
            "he the the the the the the \n",
            "he the the the the the the the \n",
            "he the the the the the the the the \n",
            "he the the the the the the the the the \n",
            "he the the the the the the the the the the \n",
            "he the the the the the the the the the the the \n",
            "he the the the the the the the the the the the the \n",
            "he the the the the the the the the the the the the the \n",
            "eleven \n",
            "eleven the \n",
            "eleven the the \n",
            "eleven the the the \n",
            "eleven the the the the \n",
            "eleven the the the the the \n",
            "eleven the the the the the the \n",
            "eleven the the the the the the the \n",
            "eleven the the the the the the the the \n",
            "eleven the the the the the the the the the \n",
            "eleven the the the the the the the the the the \n",
            "eleven the the the the the the the the the the the \n",
            "eleven the the the the the the the the the the the the \n",
            "eleven the the the the the the the the the the the the the \n",
            "Thomas \n",
            "Thomas the \n",
            "Thomas the the \n",
            "Thomas the the the \n",
            "Thomas the the the the \n",
            "Thomas the the the the the \n",
            "Thomas the the the the the the \n",
            "Thomas the the the the the the the \n",
            "Thomas the the the the the the the the \n",
            "Thomas the the the the the the the the the \n",
            "Thomas the the the the the the the the the the \n",
            "Thomas the the the the the the the the the the the \n",
            "Thomas the the the the the the the the the the the the \n",
            "Thomas the the the the the the the the the the the the the \n",
            "Bum \n",
            "Bum the \n",
            "Bum the the \n",
            "Bum the the the \n",
            "Bum the the the the \n",
            "Bum the the the the the \n",
            "Bum the the the the the the \n",
            "Bum the the the the the the the \n",
            "Bum the the the the the the the the \n",
            "Bum the the the the the the the the the \n",
            "Bum the the the the the the the the the the \n",
            "Bum the the the the the the the the the the the \n",
            "Bum the the the the the the the the the the the the \n",
            "Bum the the the the the the the the the the the the the \n",
            "Jesus \n",
            "Jesus the \n",
            "Jesus the the \n",
            "Jesus the the the \n",
            "Jesus the the the the \n",
            "Jesus the the the the the \n",
            "Jesus the the the the the the \n",
            "Jesus the the the the the the the \n",
            "Jesus the the the the the the the the \n",
            "Jesus the the the the the the the the the \n",
            "Jesus the the the the the the the the the the \n",
            "Jesus the the the the the the the the the the the \n",
            "Jesus the the the the the the the the the the the the \n",
            "Jesus the the the the the the the the the the the the the \n",
            "its \n",
            "its the \n",
            "its the the \n",
            "its the the the \n",
            "its the the the the \n",
            "its the the the the the \n",
            "its the the the the the the \n",
            "its the the the the the the the \n",
            "its the the the the the the the the \n",
            "its the the the the the the the the the \n",
            "its the the the the the the the the the the \n",
            "its the the the the the the the the the the the \n",
            "its the the the the the the the the the the the the \n",
            "its the the the the the the the the the the the the the \n",
            "Catalan \n",
            "Catalan the \n",
            "Catalan the the \n",
            "Catalan the the the \n",
            "Catalan the the the the \n",
            "Catalan the the the the the \n",
            "Catalan the the the the the the \n",
            "Catalan the the the the the the the \n",
            "Catalan the the the the the the the the \n",
            "Catalan the the the the the the the the the \n",
            "Catalan the the the the the the the the the the \n",
            "Catalan the the the the the the the the the the the \n",
            "Catalan the the the the the the the the the the the the \n",
            "Catalan the the the the the the the the the the the the the \n",
            "a \n",
            "a the \n",
            "a the the \n",
            "a the the the \n",
            "a the the the the \n",
            "a the the the the the \n",
            "a the the the the the the \n",
            "a the the the the the the the \n",
            "a the the the the the the the the \n",
            "a the the the the the the the the the \n",
            "a the the the the the the the the the the \n",
            "a the the the the the the the the the the the \n",
            "a the the the the the the the the the the the the \n",
            "a the the the the the the the the the the the the the \n",
            "Alan \n",
            "Alan the \n",
            "Alan the the \n",
            "Alan the the the \n",
            "Alan the the the the \n",
            "Alan the the the the the \n",
            "Alan the the the the the the \n",
            "Alan the the the the the the the \n",
            "Alan the the the the the the the the \n",
            "Alan the the the the the the the the the \n",
            "Alan the the the the the the the the the the \n",
            "Alan the the the the the the the the the the the \n",
            "Alan the the the the the the the the the the the the \n",
            "Alan the the the the the the the the the the the the the \n",
            "$ \n",
            "$ the \n",
            "$ the the \n",
            "$ the the the \n",
            "$ the the the the \n",
            "$ the the the the the \n",
            "$ the the the the the the \n",
            "$ the the the the the the the \n",
            "$ the the the the the the the the \n",
            "$ the the the the the the the the the \n",
            "$ the the the the the the the the the the \n",
            "$ the the the the the the the the the the the \n",
            "$ the the the the the the the the the the the the \n",
            "$ the the the the the the the the the the the the the \n",
            "A \n",
            "A the \n",
            "A the the \n",
            "A the the the \n",
            "A the the the the \n",
            "A the the the the the \n",
            "A the the the the the the \n",
            "A the the the the the the the \n",
            "A the the the the the the the the \n",
            "A the the the the the the the the the \n",
            "A the the the the the the the the the the \n",
            "A the the the the the the the the the the the \n",
            "A the the the the the the the the the the the the \n",
            "A the the the the the the the the the the the the the \n",
            "Durrant \n",
            "Durrant the \n",
            "Durrant the the \n",
            "Durrant the the the \n",
            "Durrant the the the the \n",
            "Durrant the the the the the \n",
            "Durrant the the the the the the \n",
            "Durrant the the the the the the the \n",
            "Durrant the the the the the the the the \n",
            "Durrant the the the the the the the the the \n",
            "Durrant the the the the the the the the the the \n",
            "Durrant the the the the the the the the the the the \n",
            "Durrant the the the the the the the the the the the the \n",
            "Durrant the the the the the the the the the the the the the \n",
            "\"Keep \n",
            "\"Keep the \n",
            "\"Keep the the \n",
            "\"Keep the the the \n",
            "\"Keep the the the the \n",
            "\"Keep the the the the the \n",
            "\"Keep the the the the the the \n",
            "\"Keep the the the the the the the \n",
            "\"Keep the the the the the the the the \n",
            "\"Keep the the the the the the the the the \n",
            "\"Keep the the the the the the the the the the \n",
            "\"Keep the the the the the the the the the the the \n",
            "\"Keep the the the the the the the the the the the the \n",
            "\"Keep the the the the the the the the the the the the the \n",
            "In \n",
            "In the \n",
            "In the the \n",
            "In the the the \n",
            "In the the the the \n",
            "In the the the the the \n",
            "In the the the the the the \n",
            "In the the the the the the the \n",
            "In the the the the the the the the \n",
            "In the the the the the the the the the \n",
            "In the the the the the the the the the the \n",
            "In the the the the the the the the the the the \n",
            "In the the the the the the the the the the the the \n",
            "In the the the the the the the the the the the the the \n",
            "1990 \n",
            "1990 the \n",
            "1990 the the \n",
            "1990 the the the \n",
            "1990 the the the the \n",
            "1990 the the the the the \n",
            "1990 the the the the the the \n",
            "1990 the the the the the the the \n",
            "1990 the the the the the the the the \n",
            "1990 the the the the the the the the the \n",
            "1990 the the the the the the the the the the \n",
            "1990 the the the the the the the the the the the \n",
            "1990 the the the the the the the the the the the the \n",
            "1990 the the the the the the the the the the the the the \n",
            "she \n",
            "she the \n",
            "she the the \n",
            "she the the the \n",
            "she the the the the \n",
            "she the the the the the \n",
            "she the the the the the the \n",
            "she the the the the the the the \n",
            "she the the the the the the the the \n",
            "she the the the the the the the the the \n",
            "she the the the the the the the the the the \n",
            "she the the the the the the the the the the the \n",
            "she the the the the the the the the the the the the \n",
            "she the the the the the the the the the the the the the \n",
            "hey \n",
            "hey the \n",
            "hey the the \n",
            "hey the the the \n",
            "hey the the the the \n",
            "hey the the the the the \n",
            "hey the the the the the the \n",
            "hey the the the the the the the \n",
            "hey the the the the the the the the \n",
            "hey the the the the the the the the the \n",
            "hey the the the the the the the the the the \n",
            "hey the the the the the the the the the the the \n",
            "hey the the the the the the the the the the the the \n",
            "hey the the the the the the the the the the the the the \n",
            "Critics \n",
            "Critics the \n",
            "Critics the the \n",
            "Critics the the the \n",
            "Critics the the the the \n",
            "Critics the the the the the \n",
            "Critics the the the the the the \n",
            "Critics the the the the the the the \n",
            "Critics the the the the the the the the \n",
            "Critics the the the the the the the the the \n",
            "Critics the the the the the the the the the the \n",
            "Critics the the the the the the the the the the the \n",
            "Critics the the the the the the the the the the the the \n",
            "Critics the the the the the the the the the the the the the \n",
            "audio \n",
            "audio the \n",
            "audio the the \n",
            "audio the the the \n",
            "audio the the the the \n",
            "audio the the the the the \n",
            "audio the the the the the the \n",
            "audio the the the the the the the \n",
            "audio the the the the the the the the \n",
            "audio the the the the the the the the the \n",
            "audio the the the the the the the the the the \n",
            "audio the the the the the the the the the the the \n",
            "audio the the the the the the the the the the the the \n",
            "audio the the the the the the the the the the the the the \n",
            "decided \n",
            "decided the \n",
            "decided the the \n",
            "decided the the the \n",
            "decided the the the the \n",
            "decided the the the the the \n",
            "decided the the the the the the \n",
            "decided the the the the the the the \n",
            "decided the the the the the the the the \n",
            "decided the the the the the the the the the \n",
            "decided the the the the the the the the the the \n",
            "decided the the the the the the the the the the the \n",
            "decided the the the the the the the the the the the the \n",
            "decided the the the the the the the the the the the the the \n",
            "he \n",
            "he the \n",
            "he the the \n",
            "he the the the \n",
            "he the the the the \n",
            "he the the the the the \n",
            "he the the the the the the \n",
            "he the the the the the the the \n",
            "he the the the the the the the the \n",
            "he the the the the the the the the the \n",
            "he the the the the the the the the the the \n",
            "he the the the the the the the the the the the \n",
            "he the the the the the the the the the the the the \n",
            "he the the the the the the the the the the the the the \n",
            "officially \n",
            "officially the \n",
            "officially the the \n",
            "officially the the the \n",
            "officially the the the the \n",
            "officially the the the the the \n",
            "officially the the the the the the \n",
            "officially the the the the the the the \n",
            "officially the the the the the the the the \n",
            "officially the the the the the the the the the \n",
            "officially the the the the the the the the the the \n",
            "officially the the the the the the the the the the the \n",
            "officially the the the the the the the the the the the the \n",
            "officially the the the the the the the the the the the the the \n",
            "Su \n",
            "Su the \n",
            "Su the the \n",
            "Su the the the \n",
            "Su the the the the \n",
            "Su the the the the the \n",
            "Su the the the the the the \n",
            "Su the the the the the the the \n",
            "Su the the the the the the the the \n",
            "Su the the the the the the the the the \n",
            "Su the the the the the the the the the the \n",
            "Su the the the the the the the the the the the \n",
            "Su the the the the the the the the the the the the \n",
            "Su the the the the the the the the the the the the the \n",
            "English \n",
            "English the \n",
            "English the the \n",
            "English the the the \n",
            "English the the the the \n",
            "English the the the the the \n",
            "English the the the the the the \n",
            "English the the the the the the the \n",
            "English the the the the the the the the \n",
            "English the the the the the the the the the \n",
            "English the the the the the the the the the the \n",
            "English the the the the the the the the the the the \n",
            "English the the the the the the the the the the the the \n",
            "English the the the the the the the the the the the the the \n",
            "she \n",
            "she the \n",
            "she the the \n",
            "she the the the \n",
            "she the the the the \n",
            "she the the the the the \n",
            "she the the the the the the \n",
            "she the the the the the the the \n",
            "she the the the the the the the the \n",
            "she the the the the the the the the the \n",
            "she the the the the the the the the the the \n",
            "she the the the the the the the the the the the \n",
            "she the the the the the the the the the the the the \n",
            "she the the the the the the the the the the the the the \n",
            "The \n",
            "The the \n",
            "The the the \n",
            "The the the the \n",
            "The the the the the \n",
            "The the the the the the \n",
            "The the the the the the the \n",
            "The the the the the the the the \n",
            "The the the the the the the the the \n",
            "The the the the the the the the the the \n",
            "The the the the the the the the the the the \n",
            "The the the the the the the the the the the the \n",
            "The the the the the the the the the the the the the \n",
            "The the the the the the the the the the the the the the \n",
            "march \n",
            "march the \n",
            "march the the \n",
            "march the the the \n",
            "march the the the the \n",
            "march the the the the the \n",
            "march the the the the the the \n",
            "march the the the the the the the \n",
            "march the the the the the the the the \n",
            "march the the the the the the the the the \n",
            "march the the the the the the the the the the \n",
            "march the the the the the the the the the the the \n",
            "march the the the the the the the the the the the the \n",
            "march the the the the the the the the the the the the the \n",
            "unknown \n",
            "unknown the \n",
            "unknown the the \n",
            "unknown the the the \n",
            "unknown the the the the \n",
            "unknown the the the the the \n",
            "unknown the the the the the the \n",
            "unknown the the the the the the the \n",
            "unknown the the the the the the the the \n",
            "unknown the the the the the the the the the \n",
            "unknown the the the the the the the the the the \n",
            "unknown the the the the the the the the the the the \n",
            "unknown the the the the the the the the the the the the \n",
            "unknown the the the the the the the the the the the the the \n",
            "was \n",
            "was the \n",
            "was the the \n",
            "was the the the \n",
            "was the the the the \n",
            "was the the the the the \n",
            "was the the the the the the \n",
            "was the the the the the the the \n",
            "was the the the the the the the the \n",
            "was the the the the the the the the the \n",
            "was the the the the the the the the the the \n",
            "was the the the the the the the the the the the \n",
            "was the the the the the the the the the the the the \n",
            "was the the the the the the the the the the the the the \n",
            "Andrew \n",
            "Andrew the \n",
            "Andrew the the \n",
            "Andrew the the the \n",
            "Andrew the the the the \n",
            "Andrew the the the the the \n",
            "Andrew the the the the the the \n",
            "Andrew the the the the the the the \n",
            "Andrew the the the the the the the the \n",
            "Andrew the the the the the the the the the \n",
            "Andrew the the the the the the the the the the \n",
            "Andrew the the the the the the the the the the the \n",
            "Andrew the the the the the the the the the the the the \n",
            "Andrew the the the the the the the the the the the the the \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYFeIkAhePLU"
      },
      "source": [
        "def answer_generation(ques):\n",
        "  sentence = []\n",
        "  for word in ques.split():\n",
        "      if word in word2idx:\n",
        "        sentence.extend([word2idx[word]])\n",
        "      else:\n",
        "        word = word.lower()\n",
        "        if word in word2idx:\n",
        "          sentence.extend([word2idx[word]])\n",
        "        else:\n",
        "          if word == 'eos':\n",
        "            sentence.extend([word2idx['end']])\n",
        "          else:\n",
        "            sentence.extend([word2idx['unknown']])\n",
        "  ques_len = 50 \n",
        "  ques_feature = np.zeros((1, 50))\n",
        "  ques_feature[:,0:len(sentence)] = np.array(sentence)[:50]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWpLBPfVfS0_",
        "outputId": "beb4c75e-f47f-4b98-b72f-ed846090a50c"
      },
      "source": [
        "answer_generation(\"What is your name ?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}